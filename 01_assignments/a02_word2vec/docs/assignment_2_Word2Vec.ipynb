{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Word2Vec Representations & Sigmoid Classification\n",
    "\n",
    "\n",
    "In this assignment you will:\n",
    "\n",
    "1. Explore semantic properties captured by the 300‑dimensional **Google News Word2Vec** model.\n",
    "2. Build a *sigmoid (logistic) classifier* that operates **directly on pre‑trained word vectors**.\n",
    "\n",
    "The goal is to deepen your intuition for distributional semantics and to give you hands‑on experience using dense word representations as features for a simple supervised task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download and Extract the GoogleNews Vectors  \n",
    "Download the pre-trained Word2Vec model from the following Google Drive link and move it to the current folder (i.e. `Assignment_2/`):  \n",
    "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing  \n",
    "Once you have the `.gz` file, run the code below to extract `GoogleNews-vectors-negative300.bin.gz` and print basic summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import extract_and_summary_gnews\n",
    "\n",
    "extract_and_summary_gnews()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Word Analogy Task\n",
    "\n",
    "The `get_vector(word: str)` function retrieves the word2vec embedding for a given word, and `top_k_neighbours(target_vec, k)` returns the top *k* nearest neighbours in the vocabulary for a specified vector. Your goal is to implement the `analogy` function: given words **a**, **b**, and **c**, it should return the top *k* words **d** such that  \n",
    "$$\\mathbf{v}_b - \\mathbf{v}_a + \\mathbf{v}_c \\approx \\mathbf{v}_d.$$\n",
    "Once you complete the `analogy` function, running the code block will demonstrate several example word analogies. **Do not** change any existing lines, only fill in the sections marked: \n",
    "```\n",
    "############## YOUR CODE HERE ##############\n",
    "#                                           \n",
    "#                                           \n",
    "############## YOUR CODE HERE ##############\n",
    "```\n",
    "Some hints are provided above the marked sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the pre-trained GoogleNews Word2Vec model\n",
    "model: KeyedVectors = KeyedVectors.load_word2vec_format(\n",
    "    \"GoogleNews-vectors-negative300.bin\",\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "def get_vector(word: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return the 300-dimensional vector for `word` from the pre-loaded `model`.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If `word` is out-of-vocabulary (OOV).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # In Gensim 4.x, indexing directly: model[word] returns a numpy.ndarray of shape (300,)\n",
    "        return model[word]\n",
    "    except KeyError:\n",
    "        # If the word is not in the model’s vocabulary, propagate KeyError with a clearer message\n",
    "        raise KeyError(f\"'{word}' not found in Word2Vec vocabulary.\")\n",
    "\n",
    "def top_k_neighbours(target_vec: np.ndarray, k: int) -> List[tuple]:\n",
    "    \"\"\"\n",
    "    Return the top `k` most similar words to the given vector `target_vec`,\n",
    "    as a list of (word, similarity_score) tuples.\n",
    "\n",
    "    Args:\n",
    "        target_vec: A numpy array (shape (300,)) representing the target word vector.\n",
    "        k: Number of nearest neighbors to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples: [(word1, similarity1), (word2, similarity2), ...].\n",
    "    \"\"\"\n",
    "    return model.similar_by_vector(target_vec, topn=k)\n",
    "\n",
    "\n",
    "\n",
    "def analogy(a: str, b: str, c: str, k: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Solve the analogy a : b :: c : d by finding the top-k words d whose vectors\n",
    "    are closest to (vec_b - vec_a + vec_c), excluding a, b, and c themselves.\n",
    "\n",
    "    Returns:\n",
    "        A list of the top `k` predicted words (strings).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for w in (a, b, c):\n",
    "        if w not in model.key_to_index:\n",
    "            raise KeyError(f\"Word '{w}' not found in the Word2Vec vocabulary.\")\n",
    "    \n",
    "    # Retrieve the vectors:\n",
    "    vec_a = model[a]\n",
    "    vec_b = model[b]\n",
    "    vec_c = model[c]\n",
    "\n",
    "\n",
    "    # Compute the target vector: target_vec = vec_b - vec_a + vec_c\n",
    "    # Get the top (k + 3) similar words to `target_vec` (requesting a few extra candidates (k+3) so that after filtering out you still have `k` words.)\n",
    "    # Then filter out any occurrences of `a`, `b`, or `c` and return a list of the top `k` words\n",
    "    ############## YOUR CODE HERE ##############\n",
    "    #\n",
    "    # analogy_exclude_self: List[str] = ?\n",
    "    #\n",
    "    ############## YOUR CODE HERE ##############\n",
    "\n",
    "    return analogy_exclude_self\n",
    "    \n",
    "\n",
    "\n",
    "print(\"Top 5 similar words to `man` →\", top_k_neighbours('man', 5))\n",
    "print(\"Top 5 similar words to `fance` →\", top_k_neighbours('france', 5))\n",
    "\n",
    "# Example 1: Gender relation (king : man :: queen : ?)\n",
    "analogy_1 = analogy(\"king\", \"man\", \"queen\", k=5)\n",
    "print(\"Analogy 'king - man = queen - ?' →\", analogy_1)\n",
    "\n",
    "# Example 2: Capital–country relation (Paris : France :: Tokyo : ?)\n",
    "analogy_2 = analogy(\"paris\", \"france\", \"tokyo\", k=5)\n",
    "print(\"Analogy 'paris - france = tokyo - ?' →\", analogy_2)\n",
    "\n",
    "# Example 3: Singular–plural (car : cars :: child : ?)\n",
    "analogy_3 = analogy(\"car\", \"cars\", \"child\", k=5)\n",
    "print(\"Analogy 'car - cars = child - ?' →\", analogy_3)\n",
    "\n",
    "# Example 4: Verb tense (run : running :: swim : ?)\n",
    "analogy_4 = analogy(\"run\", \"running\", \"swim\", k=5)\n",
    "print(\"Analogy 'run - running = swim - ?' →\", analogy_4)\n",
    "\n",
    "\n",
    "# Example 5: Currency relation (dollar : USA :: yen : ?)\n",
    "analogy_5 = analogy(\"dollar\", \"usa\", \"yen\", k=5)\n",
    "print(\"Analogy 'dollar - usa = yen - ?' →\", analogy_5)\n",
    "\n",
    "\n",
    "# Example 6: Profession–person (doctor : hospital :: teacher : ?)\n",
    "analogy_6 = analogy(\"doctor\", \"hospital\", \"teacher\", k=5)\n",
    "print(\"Analogy 'doctor - hospital = teacher - ?' →\", analogy_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train a Binary Sentiment Classifier with Word2Vec Embeddings\n",
    "\n",
    "In this step, you will download the [Opinion Lexicon](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html) (Hu & Liu, 2004), which contains two lists of words labeled “positive” and “negative” based on consumer reviews. The next cell uses the helper function `load_lexicon_and_filter` to download and parse the positive and negative word lists from the Opinion Lexicon, then filter out any words that do not appear in the pre-trained GoogleNews Word2Vec vocabulary. The codes then combine them into a single dataset of words with binary labels (1 = positive, 0 = negative) and construct a feature matrix **X** where each row is the 300-dimensional Word2Vec vector for that word. The **X** and the corresponding labels **y** are splitted into an 80% training set and a 20% test set, stratified by label.\n",
    "\n",
    "\n",
    "The task for you is to train a logistic regression model on X_train and y_train, and then produce predictions on X_test. Function `result_summary` will summarize the evalution result on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from data_processing import load_lexicon_and_filter, result_summary\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Load pre-trained Word2Vec embeddings and filter lexicon-based word lists\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "model: KeyedVectors = KeyedVectors.load_word2vec_format(\n",
    "    \"GoogleNews-vectors-negative300.bin\",\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "# Filter positive and negative words using the pre-trained embeddings\n",
    "filtered_positive, filtered_negative = load_lexicon_and_filter(model)\n",
    "\n",
    "# Combine positive and negative words into a single list with binary labels\n",
    "all_words = filtered_positive + filtered_negative\n",
    "labels = [1] * len(filtered_positive) + [0] * len(filtered_negative)\n",
    "n_samples = len(all_words)\n",
    "\n",
    "print(f\"[DATA] Total samples = {n_samples} (pos={len(filtered_positive)}, neg={len(filtered_negative)})\")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Build feature matrix X (word embeddings) and label vector y\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "embedding_dim = model.vector_size\n",
    "\n",
    "# Initialize an empty feature matrix of shape (n_samples, embedding_dim)\n",
    "X = np.zeros((n_samples, embedding_dim), dtype=np.float32)\n",
    "for i, word in enumerate(all_words):\n",
    "    X[i, :] = model[word]  # Fetch the 300-dim vector for each word\n",
    "\n",
    "# Convert the label list to a NumPy array of shape (n_samples,)\n",
    "y = np.array(labels, dtype=np.int64)\n",
    "\n",
    "print(f\"[DATA] Feature matrix X shape = {X.shape}, Label vector y shape = {y.shape}\\n\")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "idxs = np.arange(n_samples)\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X, y, idxs,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"[SPLIT] X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"[SPLIT] X_test : {X_test.shape}, y_test : {y_test.shape}\\n\")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Train a Logistic Regression classifier to distinguish positive vs. negative words\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "print(\"[TRAIN] Fitting Logistic Regression...\")\n",
    "\n",
    "\n",
    "\n",
    "# Train a logistic regression model on X_train and y_train\n",
    "# The produce predictions on X_test and store them in y_pred_logreg\n",
    "############## YOUR CODE HERE ##############\n",
    "#\n",
    "# y_pred_logreg = ?\n",
    "#\n",
    "############## YOUR CODE HERE ##############\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Evaluate the classifier by summarizing results on the test set\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "result_summary(y_test, y_pred_logreg, all_words, idx_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze and Discuss Misclassified Words\n",
    "\n",
    "After running `result_summary`, you will receive a list of words that your logistic regression model classified incorrectly. For each misclassified word, consider:\n",
    "\n",
    "- **Boundary Cases**: Is the word inherently ambiguous or context-dependent?  \n",
    "- **Polysemy and Context**: Does the word have multiple meanings that Word2Vec embeddings might conflate?  \n",
    "- **Nearest Neighbors**: How do the word’s closest vectors in the embedding space influence its classification?  \n",
    "\n",
    "By exploring these questions, you’ll gain intuition about how Word2Vec encodes semantic and sentiment information—and why certain words may be misclassified even when their true sentiment is clear."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
