{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Word2Vec Representations & Sigmoid Classification\n",
    "\n",
    "\n",
    "In this assignment you will:\n",
    "\n",
    "1. Explore semantic properties captured by the 300‑dimensional **Google News Word2Vec** model.\n",
    "2. Build a *sigmoid (logistic) classifier* that operates **directly on pre‑trained word vectors**.\n",
    "\n",
    "The goal is to deepen your intuition for distributional semantics and to give you hands‑on experience using dense word representations as features for a simple supervised task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download and Extract the GoogleNews Vectors  \n",
    "Download the pre-trained Word2Vec model from the following Google Drive link and move it to the current folder (i.e. `Assignment_2/`):  \n",
    "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing  \n",
    "Once you have the `.gz` file, run the code below to extract `GoogleNews-vectors-negative300.bin.gz` and print basic summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressing `.gz` → `.bin` (this may take a few minutes)…\n",
      "Decompressed to: /Users/ryanabsar/Documents/02_Education/01_ICL_BA/01_Modules/03_Summer Semester/03_genai_llm/genai-llm-imperial-ba/01_assignments/a02_word2vec/GoogleNews-vectors-negative300.bin\n",
      "\n",
      "=== Summary Statistics ===\n",
      "• Total vocabulary size: 3,000,000 words\n",
      "• Vector dimensionality: 300\n",
      "\n",
      "Loading a small sample (limit=10) to display a few word–vector snippets…\n",
      "• Sample words loaded (first 10): ['</s>', 'in', 'for', 'that', 'is', 'on', '##', 'The', 'with', 'said']\n",
      "\n",
      "• Vector snippets for the first 5 sample words:\n",
      "    </s>            → [0.001129150390625, -0.000896453857421875, 0.0003185272216796875, 0.00153350830078125, 0.00110626220703125, -0.00140380859375] …\n",
      "    in              → [0.0703125, 0.0869140625, 0.087890625, 0.0625, 0.0693359375, -0.10888671875] …\n",
      "    for             → [-0.01177978515625, -0.04736328125, 0.044677734375, 0.0634765625, -0.0181884765625, -0.06396484375] …\n",
      "    that            → [-0.0157470703125, -0.0283203125, 0.08349609375, 0.05029296875, -0.1103515625, 0.03173828125] …\n",
      "    is              → [0.007049560546875, -0.0732421875, 0.171875, 0.0225830078125, -0.1328125, 0.1982421875] …\n"
     ]
    }
   ],
   "source": [
    "from data_processing import extract_and_summary_gnews\n",
    "\n",
    "extract_and_summary_gnews()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Word Analogy Task\n",
    "\n",
    "The `get_vector(word: str)` function retrieves the word2vec embedding for a given word, and `top_k_neighbours(target_vec, k)` returns the top *k* nearest neighbours in the vocabulary for a specified vector. Your goal is to implement the `analogy` function: given words **a**, **b**, and **c**, it should return the top *k* words **d** such that  \n",
    "$$\\mathbf{v}_b - \\mathbf{v}_a + \\mathbf{v}_c \\approx \\mathbf{v}_d.$$\n",
    "Once you complete the `analogy` function, running the code block will demonstrate several example word analogies. **Do not** change any existing lines, only fill in the sections marked: \n",
    "```\n",
    "############## YOUR CODE HERE ##############\n",
    "#                                           \n",
    "#                                           \n",
    "############## YOUR CODE HERE ##############\n",
    "```\n",
    "Some hints are provided above the marked sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 similar words to `man` → [('woman', 0.7664012908935547), ('boy', 0.6824871301651001), ('teenager', 0.6586930155754089), ('teenage_girl', 0.6147903203964233), ('girl', 0.5921714305877686)]\n",
      "Top 5 similar words to `fance` → [('spain', 0.6375302672386169), ('french', 0.6326055526733398), ('germany', 0.6314354538917542), ('europe', 0.6264256238937378), ('italy', 0.6257959008216858)]\n",
      "Analogy 'king - man = queen - ?' → ['woman', 'girl', 'lady', 'teenage_girl', 'teenager']\n",
      "Analogy 'paris - france = tokyo - ?' → ['japan', 'hong_kong', 'japanese', 'germany', 'europe']\n",
      "Analogy 'car - cars = child - ?' → ['children', 'babies', 'newborns', 'infants', 'kids']\n",
      "Analogy 'run - running = swim - ?' → ['swimming', 'swam', 'swims', 'swimmers', 'swum']\n",
      "Analogy 'dollar - usa = yen - ?' → ['japanese', 'japan', 'india', 'uk', '¥']\n",
      "Analogy 'doctor - hospital = teacher - ?' → ['elementary', 'teachers', 'school', 'classroom', 'School']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the pre-trained GoogleNews Word2Vec model\n",
    "model: KeyedVectors = KeyedVectors.load_word2vec_format(\n",
    "    \"GoogleNews-vectors-negative300.bin\",\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "def get_vector(word: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return the 300-dimensional vector for `word` from the pre-loaded `model`.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If `word` is out-of-vocabulary (OOV).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # In Gensim 4.x, indexing directly: model[word] returns a numpy.ndarray of shape (300,)\n",
    "        return model[word]\n",
    "    except KeyError:\n",
    "        # If the word is not in the model’s vocabulary, propagate KeyError with a clearer message\n",
    "        raise KeyError(f\"'{word}' not found in Word2Vec vocabulary.\")\n",
    "\n",
    "def top_k_neighbours(target_vec: np.ndarray, k: int) -> List[tuple]:\n",
    "    \"\"\"\n",
    "    Return the top `k` most similar words to the given vector `target_vec`,\n",
    "    as a list of (word, similarity_score) tuples.\n",
    "\n",
    "    Args:\n",
    "        target_vec: A numpy array (shape (300,)) representing the target word vector.\n",
    "        k: Number of nearest neighbors to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples: [(word1, similarity1), (word2, similarity2), ...].\n",
    "    \"\"\"\n",
    "    return model.similar_by_vector(target_vec, topn=k)\n",
    "\n",
    "\n",
    "\n",
    "def analogy(a: str, b: str, c: str, k: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Solve the analogy a : b :: c : d by finding the top-k words d whose vectors\n",
    "    are closest to (vec_b - vec_a + vec_c), excluding a, b, and c themselves.\n",
    "\n",
    "    Returns:\n",
    "        A list of the top `k` predicted words (strings).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for w in (a, b, c):\n",
    "        if w not in model.key_to_index:\n",
    "            raise KeyError(f\"Word '{w}' not found in the Word2Vec vocabulary.\")\n",
    "    \n",
    "    # Retrieve the vectors:\n",
    "    vec_a = model[a]\n",
    "    vec_b = model[b]\n",
    "    vec_c = model[c]\n",
    "\n",
    "\n",
    "    # Compute the target vector: target_vec = vec_b - vec_a + vec_c\n",
    "    # Get the top (k + 3) similar words to `target_vec` (requesting a few extra candidates (k+3) so that after filtering out you still have `k` words.)\n",
    "    # Then filter out any occurrences of `a`, `b`, or `c` and return a list of the top `k` words\n",
    "    ############## YOUR CODE HERE ##############\n",
    "\n",
    "    target_vec = vec_b - vec_a + vec_c\n",
    "    top_k_candidates = top_k_neighbours(target_vec, k + 3)\n",
    "    # Filter out the words a, b, c and keep only the top k candidates\n",
    "    analogy_exclude_self = [word for word, _ in top_k_candidates if word not in (a, b, c)][:k]\n",
    "    \n",
    "    \n",
    "    ############## YOUR CODE HERE ##############\n",
    "\n",
    "    return analogy_exclude_self\n",
    "    \n",
    "\n",
    "\n",
    "print(\"Top 5 similar words to `man` →\", top_k_neighbours('man', 5))\n",
    "print(\"Top 5 similar words to `fance` →\", top_k_neighbours('france', 5))\n",
    "\n",
    "# # Example 1: Gender relation (king : man :: queen : ?)\n",
    "analogy_1 = analogy(\"king\", \"man\", \"queen\", k=5)\n",
    "print(\"Analogy 'king - man = queen - ?' →\", analogy_1)\n",
    "\n",
    "# Example 2: Capital–country relation (Paris : France :: Tokyo : ?)\n",
    "analogy_2 = analogy(\"paris\", \"france\", \"tokyo\", k=5)\n",
    "print(\"Analogy 'paris - france = tokyo - ?' →\", analogy_2)\n",
    "\n",
    "# Example 3: Singular–plural (car : cars :: child : ?)\n",
    "analogy_3 = analogy(\"car\", \"cars\", \"child\", k=5)\n",
    "print(\"Analogy 'car - cars = child - ?' →\", analogy_3)\n",
    "\n",
    "# Example 4: Verb tense (run : running :: swim : ?)\n",
    "analogy_4 = analogy(\"run\", \"running\", \"swim\", k=5)\n",
    "print(\"Analogy 'run - running = swim - ?' →\", analogy_4)\n",
    "\n",
    "\n",
    "# Example 5: Currency relation (dollar : USA :: yen : ?)\n",
    "analogy_5 = analogy(\"dollar\", \"usa\", \"yen\", k=5)\n",
    "print(\"Analogy 'dollar - usa = yen - ?' →\", analogy_5)\n",
    "\n",
    "\n",
    "# Example 6: Profession–person (doctor : hospital :: teacher : ?)\n",
    "analogy_6 = analogy(\"doctor\", \"hospital\", \"teacher\", k=5)\n",
    "print(\"Analogy 'doctor - hospital = teacher - ?' →\", analogy_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train a Binary Sentiment Classifier with Word2Vec Embeddings\n",
    "\n",
    "In this step, you will download the [Opinion Lexicon](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html) (Hu & Liu, 2004), which contains two lists of words labeled “positive” and “negative” based on consumer reviews. The next cell uses the helper function `load_lexicon_and_filter` to download and parse the positive and negative word lists from the Opinion Lexicon, then filter out any words that do not appear in the pre-trained GoogleNews Word2Vec vocabulary. The codes then combine them into a single dataset of words with binary labels (1 = positive, 0 = negative) and construct a feature matrix **X** where each row is the 300-dimensional Word2Vec vector for that word. The **X** and the corresponding labels **y** are splitted into an 80% training set and a 20% test set, stratified by label.\n",
    "\n",
    "\n",
    "The task for you is to train a logistic regression model on X_train and y_train, and then produce predictions on X_test. Function `result_summary` will summarize the evalution result on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] positive-words.txt already exists.\n",
      "[SKIP] negative-words.txt already exists.\n",
      "Filtered 2006 → 1857 positive words kept\n",
      "Filtered 4783 → 4445 negative words kept\n",
      "\n",
      "[DATA] Total samples = 6302 (pos=1857, neg=4445)\n",
      "[DATA] Feature matrix X shape = (6302, 300), Label vector y shape = (6302,)\n",
      "\n",
      "[SPLIT] X_train: (5041, 300), y_train: (5041,)\n",
      "[SPLIT] X_test : (1261, 300), y_test : (1261,)\n",
      "\n",
      "[TRAIN] Fitting Logistic Regression...\n",
      "[TRAIN] Logistic Regression fitted.\n",
      "\n",
      "=== Logistic Regression (Sigmoid) Results ===\n",
      "Accuracy: 0.9524\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.96      0.97      0.97       889\n",
      "         POS       0.93      0.91      0.92       372\n",
      "\n",
      "    accuracy                           0.95      1261\n",
      "   macro avg       0.94      0.94      0.94      1261\n",
      "weighted avg       0.95      0.95      0.95      1261\n",
      "\n",
      "\n",
      "Misclassified by Logistic Regression:\n",
      "  Word: dominate          True=POS   Pred=NEG\n",
      "  Word: overwhelming      True=NEG   Pred=POS\n",
      "  Word: subsidizes        True=POS   Pred=NEG\n",
      "  Word: indulgence        True=POS   Pred=NEG\n",
      "  Word: emphatic          True=NEG   Pred=POS\n",
      "  Word: brainy            True=POS   Pred=NEG\n",
      "  Word: fervidly          True=POS   Pred=NEG\n",
      "  Word: stranger          True=NEG   Pred=POS\n",
      "  Word: disappoint        True=NEG   Pred=POS\n",
      "  Word: stainless         True=POS   Pred=NEG\n",
      "  Word: tenderness        True=NEG   Pred=POS\n",
      "  Word: deign             True=NEG   Pred=POS\n",
      "  Word: deaf              True=NEG   Pred=POS\n",
      "  Word: smoothes          True=POS   Pred=NEG\n",
      "  Word: scandalized       True=NEG   Pred=POS\n",
      "  Word: fiery             True=POS   Pred=NEG\n",
      "  Word: envy              True=POS   Pred=NEG\n",
      "  Word: fastidiously      True=NEG   Pred=POS\n",
      "  Word: agreeableness     True=POS   Pred=NEG\n",
      "  Word: defeat            True=POS   Pred=NEG\n",
      "  Word: gaily             True=POS   Pred=NEG\n",
      "  Word: undaunted         True=POS   Pred=NEG\n",
      "  Word: intelligence      True=POS   Pred=NEG\n",
      "  Word: unbelievable      True=NEG   Pred=POS\n",
      "  Word: slammin           True=POS   Pred=NEG\n",
      "  Word: approve           True=POS   Pred=NEG\n",
      "  Word: defeated          True=POS   Pred=NEG\n",
      "  Word: cushy             True=POS   Pred=NEG\n",
      "  Word: contagious        True=NEG   Pred=POS\n",
      "  Word: tout              True=NEG   Pred=POS\n",
      "  Word: pleasurably       True=POS   Pred=NEG\n",
      "  Word: striking          True=POS   Pred=NEG\n",
      "  Word: innocuous         True=POS   Pred=NEG\n",
      "  Word: skeptically       True=NEG   Pred=POS\n",
      "  Word: refund            True=POS   Pred=NEG\n",
      "  Word: posh              True=POS   Pred=NEG\n",
      "  Word: strictly          True=NEG   Pred=POS\n",
      "  Word: intoxicate        True=NEG   Pred=POS\n",
      "  Word: illusions         True=NEG   Pred=POS\n",
      "  Word: unreachable       True=NEG   Pred=POS\n",
      "  Word: hectic            True=NEG   Pred=POS\n",
      "  Word: suffices          True=POS   Pred=NEG\n",
      "  Word: concession        True=NEG   Pred=POS\n",
      "  Word: affably           True=POS   Pred=NEG\n",
      "  Word: matte             True=NEG   Pred=POS\n",
      "  Word: defeats           True=POS   Pred=NEG\n",
      "  Word: thinner           True=POS   Pred=NEG\n",
      "  Word: improbably        True=NEG   Pred=POS\n",
      "  Word: funky             True=NEG   Pred=POS\n",
      "  Word: deference         True=POS   Pred=NEG\n",
      "  Word: incomparable      True=NEG   Pred=POS\n",
      "  Word: lorn              True=NEG   Pred=POS\n",
      "  Word: ingenuous         True=POS   Pred=NEG\n",
      "  Word: stunned           True=POS   Pred=NEG\n",
      "  Word: flirty            True=NEG   Pred=POS\n",
      "  Word: presumptuously    True=NEG   Pred=POS\n",
      "  Word: enviously         True=NEG   Pred=POS\n",
      "  Word: advocates         True=POS   Pred=NEG\n",
      "  Word: coherent          True=POS   Pred=NEG\n",
      "  Word: flutter           True=POS   Pred=NEG\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ssl\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from data_processing import load_lexicon_and_filter, result_summary\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Load pre-trained Word2Vec embeddings and filter lexicon-based word lists\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "model: KeyedVectors = KeyedVectors.load_word2vec_format(\n",
    "    \"GoogleNews-vectors-negative300.bin\",\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "# Filter positive and negative words using the pre-trained embeddings\n",
    "filtered_positive, filtered_negative = load_lexicon_and_filter(model)\n",
    "\n",
    "# Combine positive and negative words into a single list with binary labels\n",
    "all_words = filtered_positive + filtered_negative\n",
    "labels = [1] * len(filtered_positive) + [0] * len(filtered_negative)\n",
    "n_samples = len(all_words)\n",
    "\n",
    "print(f\"[DATA] Total samples = {n_samples} (pos={len(filtered_positive)}, neg={len(filtered_negative)})\")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Build feature matrix X (word embeddings) and label vector y\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "embedding_dim = model.vector_size\n",
    "\n",
    "# Initialize an empty feature matrix of shape (n_samples, embedding_dim)\n",
    "X = np.zeros((n_samples, embedding_dim), dtype=np.float32)\n",
    "for i, word in enumerate(all_words):\n",
    "    X[i, :] = model[word]  # Fetch the 300-dim vector for each word\n",
    "\n",
    "# Convert the label list to a NumPy array of shape (n_samples,)\n",
    "y = np.array(labels, dtype=np.int64)\n",
    "\n",
    "print(f\"[DATA] Feature matrix X shape = {X.shape}, Label vector y shape = {y.shape}\\n\")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "idxs = np.arange(n_samples)\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X, y, idxs,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"[SPLIT] X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"[SPLIT] X_test : {X_test.shape}, y_test : {y_test.shape}\\n\")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Train a Logistic Regression classifier to distinguish positive vs. negative words\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "print(\"[TRAIN] Fitting Logistic Regression...\")\n",
    "\n",
    "\n",
    "\n",
    "# Train a logistic regression model on X_train and y_train\n",
    "# The produce predictions on X_test and store them in y_pred_logreg\n",
    "############## YOUR CODE HERE ##############\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "\n",
    "print(\"[TRAIN] Logistic Regression fitted.\\n\")\n",
    "############## YOUR CODE HERE ##############\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Evaluate the classifier by summarizing results on the test set\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "result_summary(y_test, y_pred_logreg, all_words, idx_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze and Discuss Misclassified Words\n",
    "\n",
    "After running `result_summary`, you will receive a list of words that your logistic regression model classified incorrectly. For each misclassified word, consider:\n",
    "\n",
    "- **Boundary Cases**: Is the word inherently ambiguous or context-dependent?  \n",
    "- **Polysemy and Context**: Does the word have multiple meanings that Word2Vec embeddings might conflate?  \n",
    "- **Nearest Neighbors**: How do the word’s closest vectors in the embedding space influence its classification?  \n",
    "\n",
    "By exploring these questions, you’ll gain intuition about how Word2Vec encodes semantic and sentiment information—and why certain words may be misclassified even when their true sentiment is clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>27</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>339</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>895</td>\n",
       "      <td>366</td>\n",
       "      <td>1261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1   All\n",
       "Actual                   \n",
       "0          862   27   889\n",
       "1           33  339   372\n",
       "All        895  366  1261"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "logreg_result = pd.DataFrame({\n",
    "    \"word\": [all_words[i] for i in idx_test],\n",
    "    \"label\": y_test,\n",
    "    \"prediction\": y_pred_logreg\n",
    "})\n",
    "\n",
    "# check false positives and false negatives\n",
    "false_positives = logreg_result[(logreg_result['label'] == 0) & (logreg_result['prediction'] == 1)]\n",
    "false_negatives = logreg_result[(logreg_result['label'] == 1) & (logreg_result['prediction'] == 0)]\n",
    "\n",
    "# confusion matrix\n",
    "confusion_matrix = pd.crosstab(logreg_result['label'], logreg_result['prediction'], rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dominate</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>subsidizes</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>indulgence</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>brainy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>fervidly</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  label  prediction\n",
       "4      dominate      1           0\n",
       "26   subsidizes      1           0\n",
       "46   indulgence      1           0\n",
       "105      brainy      1           0\n",
       "107    fervidly      1           0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>overwhelming</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>emphatic</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>stranger</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>disappoint</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>tenderness</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  label  prediction\n",
       "19   overwhelming      0           1\n",
       "98       emphatic      0           1\n",
       "125      stranger      0           1\n",
       "210    disappoint      0           1\n",
       "329    tenderness      0           1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positives.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neigbour for word 'dominate': [('dominated', 0.7352245450019836), ('dominating', 0.7086454629898071), ('dominates', 0.6767717599868774), ('dominant', 0.6167248487472534), ('dominance', 0.6048017144203186)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nearest neigbour for word \\'{false_negatives['word'].iloc[0]}\\': {top_k_neighbours(false_negatives['word'].iloc[0], 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
