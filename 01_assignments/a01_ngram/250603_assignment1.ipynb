{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd0a34f",
   "metadata": {},
   "source": [
    "# Assignment 1: N-Gram Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bcfe63",
   "metadata": {},
   "source": [
    "| Name               | Student ID |\n",
    "|--------------------|------------|\n",
    "| Abdulaziz Alfaraj  | 0   |\n",
    "| Abu Monguno        | 0   |\n",
    "| Adeeb Katib        | 0   |\n",
    "| Ryan Primadi       | 06034343   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda372bd",
   "metadata": {},
   "source": [
    "The **Berkeley Restaurant Project (BeRP)** corpus is a speech-dialogue dataset collected in the early 1990s to support a conversational system for answering restaurant‐related questions in Berkeley, California. The dataset contains nearly 9000 user utterances and more than 1700 vocabularies. In this assignment you will implement bigram and trigram language models on these queries, compute the probability of given sentences, and complete given prefixes with greedy decoding. Follow the hints in each code‐block title to fill in the missing parts of each function, then run each code-block to complete the assignment. Ensure your virtual environment is activated before launching the Jupyter Notebook. For detailed instructions, see `environment_setup.md`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f53725",
   "metadata": {},
   "source": [
    "## Step 1: Download the dataset\n",
    "Run the following code block. It automatically downloads the dataset (stored in `berp_dataset.txt`) and output some summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe83573-e5d8-4a63-9bd4-fb8b840e3eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download complete\n",
      "{\n",
      "  \"num_sentences\": 8553,\n",
      "  \"num_tokens\": 70517,\n",
      "  \"vocab_size\": 1749\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from docs.data_processing import download_berp\n",
    "\n",
    "download_berp()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7f65e2-6389-4985-8e68-aaba54054a75",
   "metadata": {},
   "source": [
    "## Step 2: Implement the N-gram Language Model Class\n",
    "\n",
    "Natural language data is sparse: many valid word sequences never appear in your training corpus, which would give them zero probability under a naïve count-based model. To address this, we apply smoothing when computing probability distributions:\n",
    "\n",
    "- **Laplace (add-one) smoothing**  \n",
    "  Add 1 to each N-gram count, and increase each prefix count by the vocabulary size V.  \n",
    "\n",
    "- **Additive smoothing**  \n",
    "  More flexible: add a small constant α (e.g. 0.1) to each count, and increase each prefix count by αV:  \n",
    "\n",
    "When generating text, you convert these probabilities into a choice. We implement two sampling methods:\n",
    "\n",
    "- **Temperature sampling** re-scales the distribution by raising each probability to the power of 1/T before renormalizing. T > 1 flattens the distribution (more exploration), while T < 1 sharpens it (more exploitation).\n",
    "\n",
    "- **Greedy sampling** always selects the highest-probability next word, equivalent to temperature sampling in the limit as T $\\to$ 0, which collapses the distribution to its single most likely outcome.\n",
    "\n",
    "\n",
    "\n",
    "Below is the skeleton of the `NGram_LM` class. **Do not** change any existing lines, only fill in the sections marked: \n",
    "```\n",
    "############## YOUR CODE HERE ##############\n",
    "#                                           \n",
    "#                                           \n",
    "############## YOUR CODE HERE ##############\n",
    "```\n",
    "Some hints are provided above the marked sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ecf855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22030075187969925\n",
      "to\n"
     ]
    }
   ],
   "source": [
    "from docs.data_processing import load_data\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Optional\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "corpus = load_data()\n",
    "\n",
    "\n",
    "class NGram_LM:\n",
    "    \"\"\"\n",
    "    An N-gram language model \n",
    "\n",
    "    Usage example:\n",
    "      corpus = load_data()\n",
    "      model = NGram_LM(corpus, N=3)\n",
    "      prob_of_to = model.next_word_prob(next_word = 'to', prefix = ['i', 'want'])\n",
    "      generated_next_word = model.next_word_generation(prefix = ['i', 'want'], greedy = True)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        corpus: List[List[str]],\n",
    "        N: int = 2,\n",
    "    ):\n",
    "        self.N = N\n",
    "        self.corpus = corpus\n",
    "        self.ngram_counts = defaultdict(int)\n",
    "        \n",
    "        \n",
    "        # Count occurrences of each N-gram in the corpus\n",
    "        # After counting, self.ngram_counts should map each N-gram tuple to its frequency\n",
    "        # For example, when N=2:\n",
    "        #   {('see', 'i'): 3,\n",
    "        #    ('i', 'want'): 773,\n",
    "        #    ('want', 'to'): 556, ...}\n",
    "        ############## YOUR CODE HERE ##############\n",
    "        \n",
    "        # code for counting N-grams is below inside the same loop to calculate prefix counts      \n",
    "                  \n",
    "        ############## YOUR CODE HERE ##############\n",
    "\n",
    "        self.prefix_counts = defaultdict(int)\n",
    "        \n",
    "        # Count occurrences of each (N-1)-gram prefix\n",
    "        # After counting, self.prefix_counts should map each prefix tuple to its frequency\n",
    "        # For example, when N=2:\n",
    "        #   {('okay',): 152,\n",
    "        #    (\"let's\",): 271,\n",
    "        #    ('see',): 84,\n",
    "        #    ('i',): 2844, ...}\n",
    "        ############## YOUR CODE HERE ##############\n",
    "              \n",
    "        for sentence in self.corpus:\n",
    "            for i, word in enumerate(sentence):\n",
    "                ngram = tuple(sentence[i:i + self.N])\n",
    "                \n",
    "                # ngram prefix is the first N-1 words of the ngram\n",
    "                # e.g. if N=2, ngram_prefix = (sentence[i],)\n",
    "                ngram_prefix = ngram[:-1]\n",
    "                \n",
    "                # Count the prefix\n",
    "                self.prefix_counts[ngram_prefix] += 1\n",
    "                \n",
    "                # Count the ngram\n",
    "                if len(ngram) == self.N:\n",
    "                    self.ngram_counts[ngram] += 1\n",
    "        \n",
    "        ############## YOUR CODE HERE ##############\n",
    "\n",
    "\n",
    "        # Vocabulary size\n",
    "        self.vocab = list(set(word for sentence in self.corpus for word in sentence))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def next_word_prob(self, next_word: str, prefix: List[str], smoothing: str = 'laplace') -> float:\n",
    "        \"\"\"Compute next word probability given prefix.\"\"\"\n",
    "        assert len(prefix) == self.N - 1, \"Prefix must be of length N-1\"\n",
    "        prefix = tuple(prefix)\n",
    "        ngram = prefix + (next_word,)\n",
    "\n",
    "\n",
    "        # Compute smoothed probability P(next_word | prefix)\n",
    "        # Use self.ngram_counts, self.prefix_counts, and the selected smoothing method\n",
    "        # the smoothing methods are provided in _laplace_smoothing and _additive_smoothing\n",
    "        ############## YOUR CODE HERE ##############\n",
    "        \n",
    "        next_word_probability = 0.0\n",
    "        \n",
    "        if smoothing == 'laplace':\n",
    "            next_word_probability = self._laplace_smoothing(self.ngram_counts[ngram], self.prefix_counts[prefix])\n",
    "        elif smoothing == 'additive':\n",
    "            next_word_probability = self._additive_smoothing(self.ngram_counts[ngram], self.prefix_counts[prefix])\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported smoothing method. Use 'laplace' or 'additive'.\")\n",
    "        \n",
    "        ############## YOUR CODE HERE ##############\n",
    "\n",
    "        return next_word_probability\n",
    "        \n",
    "\n",
    "    def next_word_generation(self, prefix: list, temperature: float = 1.0, greedy: bool = False, smoothing: str = 'laplace', seed: int = 1234):\n",
    "        \n",
    "        assert len(prefix) == self.N - 1, \"Prefix must be of length N-1\" \n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        prefix = tuple(prefix)\n",
    "        \n",
    "        \n",
    "        # sample the next word conditional on the prefix\n",
    "        # you need to implement the temperature sampling and greedy sampling\n",
    "        # return the sampled next_word\n",
    "        ############## YOUR CODE HERE ##############\n",
    "        \n",
    "        prob = np.array([self.next_word_prob(word, prefix, smoothing) for word in self.vocab])\n",
    "        \n",
    "        if greedy:\n",
    "            next_word = self.vocab[np.argmax(prob)]\n",
    "        else:\n",
    "        # re-scales the distribution by raising each probability to the power of 1/T before renormalizing. \n",
    "        # T > 1 flattens the distribution (more exploration), while T < 1 sharpens it (more exploitation).\n",
    "            # normalise the probabilities\n",
    "            norm_prob = prob / np.sum(prob)\n",
    "            \n",
    "            # rescale to the power of 1/T before renormalizing\n",
    "            rescale = np.power(norm_prob, 1.0 / temperature)  \n",
    "            \n",
    "            # renomrlise the probabilities\n",
    "            re_norm = rescale / np.sum(rescale)  \n",
    "            \n",
    "            # sample the next word from the vocabulary based on the re-normalized probabilities\n",
    "            next_word = np.random.choice(self.vocab, p=re_norm)\n",
    "            \n",
    "        ############## YOUR CODE HERE ##############\n",
    "\n",
    "        return next_word\n",
    "\n",
    "\n",
    "\n",
    "    def _laplace_smoothing(self, ngram_count: int, prefix_count: int) -> float:\n",
    "        \"\"\"Laplace smoothing (add-one).\"\"\"\n",
    "        return (ngram_count + 1) / (prefix_count + self.vocab_size)\n",
    "\n",
    "    def _additive_smoothing(self, ngram_count: int, prefix_count: int, alpha: float = 0.1) -> float:\n",
    "        \"\"\"Additive smoothing with parameter alpha.\"\"\"\n",
    "        return (ngram_count + alpha) / (prefix_count + alpha * self.vocab_size)\n",
    "\n",
    "\n",
    "# check the result\n",
    "bigram_model = NGram_LM(corpus, N=2)\n",
    "trigram_model = NGram_LM(corpus, N=3)\n",
    "print(trigram_model.next_word_prob(next_word = 'to', prefix = ['i', 'want']))\n",
    "print(trigram_model.next_word_generation(prefix = ['i', 'want'], greedy = True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1d1e89",
   "metadata": {},
   "source": [
    "## Step 3: Sentence Probability Evaluation\n",
    "\n",
    "Implement a function that computes the log-probability of a full sentence under an N-gram model. Then, using both bigram (N=2) and trigram (N=3) versions of your `NGram_LM`, calculate the log-probability of each sentence given the prefix `['i', 'want']`:\n",
    "\n",
    "1. \"i want to eat on saturday night\"  \n",
    "2. \"i want to go to a restaurant\"  \n",
    "4. \"i want its sushis\"  \n",
    "5. \"i want to eat on monday berkeley\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f842b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bigram Model ---\n",
      "Sentence: 'i want to eat on saturday night'\n",
      "Log-Probability: -13.7088\n",
      "\n",
      "Sentence: 'i want to go to a restaurant'\n",
      "Log-Probability: -13.7634\n",
      "\n",
      "Sentence: 'i want its sushis'\n",
      "Log-Probability: -15.3997\n",
      "\n",
      "Sentence: 'i want to eat on monday berkeley'\n",
      "Log-Probability: -17.7053\n",
      "\n",
      "--- Trigram Model ---\n",
      "Sentence: 'i want to eat on saturday night'\n",
      "Log-Probability: -15.8493\n",
      "\n",
      "Sentence: 'i want to go to a restaurant'\n",
      "Log-Probability: -15.1558\n",
      "\n",
      "Sentence: 'i want its sushis'\n",
      "Log-Probability: -15.3517\n",
      "\n",
      "Sentence: 'i want to eat on monday berkeley'\n",
      "Log-Probability: -19.1640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sentence_log_probability(\n",
    "    model: NGram_LM,\n",
    "    prefix: List[str],\n",
    "    sentence: List[str],\n",
    "    smoothing: str = 'laplace'\n",
    ") -> float:\n",
    "    \n",
    "    # Compute the total log-probability of `sentence` under `model`, starting from the given `prefix` and using the specified smoothing.\n",
    "    # Hint: accumulate log(next_word_prob) for each word in `sentence`\n",
    "    ############## YOUR CODE HERE ##############\n",
    "    \n",
    "    #initialize log probability\n",
    "    log_prob = 0.0\n",
    "    # keep track of the prefix history\n",
    "    prefix_history = prefix.copy()\n",
    "    \n",
    "    # print(f\"Calculating log probability for sentence: {' '.join(sentence)}\")\n",
    "    for word in sentence:\n",
    "        # get the prefix of the last N-1 words\n",
    "        current_prefix = prefix_history[-(model.N - 1):] \n",
    "        \n",
    "        # print(f\"Current prefix: {current_prefix}, Next word: {word}\")\n",
    "        \n",
    "        # calculate the probability of the next word given the current prefix\n",
    "        prob = model.next_word_prob(word, current_prefix, smoothing)\n",
    "        \n",
    "        # print(f\"Probability of '{word}' given prefix {current_prefix}: {prob:.4f}\")\n",
    "\n",
    "        log_prob += np.log(prob)\n",
    "        \n",
    "        prefix_history.append(word)\n",
    "    ############## YOUR CODE HERE ##############\n",
    "    return log_prob\n",
    "\n",
    "\n",
    "prefix = ['i', 'want']\n",
    "sentences = [\n",
    "    \"i want to eat on saturday night\",\n",
    "    \"i want to go to a restaurant\",\n",
    "    \"i want its sushis\",\n",
    "    \"i want to eat on monday berkeley\",\n",
    "]\n",
    "\n",
    "for model_name, model in [('Bigram', bigram_model), ('Trigram', trigram_model)]:\n",
    "    print(f\"--- {model_name} Model ---\")\n",
    "    for s in sentences:\n",
    "        words = s.split()[len(prefix):]\n",
    "        log_p = sentence_log_probability(model, prefix, words)\n",
    "        print(f\"Sentence: '{s}'\\nLog-Probability: {log_p:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd726d6",
   "metadata": {},
   "source": [
    "## Step 4: Next-Word Generation with Greedy and Temperature Sampling\n",
    "\n",
    "Create a function that, given a prefix, produces a sequence of next words using your `NGram_LM`. Then, for both the bigram (N=2) and trigram (N=3) models, generate five-word continuations of the prefix `['i', 'want']` using (a) greedy sampling and (b) temperature-based sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dea7605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bigram (greedy) ---\n",
      "Seed 1: i want to eat on saturday night\n",
      "Seed 2: i want to eat on saturday night\n",
      "Seed 3: i want to eat on saturday night\n",
      "\n",
      "--- Bigram (temperature=0.5) ---\n",
      "Seed 1: i want to eat on wednesday particularly\n",
      "Seed 2: i want to eat on wednesday come\n",
      "Seed 3: i want to eat lunch nineteen (i)\n",
      "\n",
      "--- Trigram (greedy) ---\n",
      "Seed 1: i want to eat on a sunday\n",
      "Seed 2: i want to eat on a sunday\n",
      "Seed 3: i want to eat on a sunday\n",
      "\n",
      "--- Trigram (temperature=0.5) ---\n",
      "Seed 1: i want to eat on style croissant\n",
      "Seed 2: i want to eat on sundays screaming\n",
      "Seed 3: i want to eat lunch midprice either\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def m_step_sentence_generation(model: NGram_LM, prefix: list, m: int, greedy: bool = False, temperature: float = 1.0, seed: int = 1234):\n",
    "    \n",
    "    \n",
    "    # Generate m next words from `model`, starting from `prefix` and using the specified sampling method\n",
    "    ############## YOUR CODE HERE ##############\n",
    "    \n",
    "    prefix_history = prefix.copy()\n",
    "    \n",
    "    for _ in range(m):\n",
    "        current_prefix = prefix_history[-(model.N - 1):]\n",
    "        generated_word = model.next_word_generation(prefix=current_prefix,greedy=greedy, temperature=temperature, seed=seed)\n",
    "        prefix_history.append(generated_word)\n",
    "    generated_sentence = prefix_history\n",
    "    \n",
    "    ############## YOUR CODE HERE ##############\n",
    "    return generated_sentence[len(list(prefix)):]\n",
    "\n",
    "\n",
    "prefix = ['i', 'want']\n",
    "m = 5\n",
    "seeds = [1, 2, 3]\n",
    "\n",
    "# Scenarios: (description, model, greedy flag, temperature)\n",
    "tests = [\n",
    "    (\"Bigram (greedy)\",     bigram_model, True,  0.5),\n",
    "    (\"Bigram (temperature=0.5)\", bigram_model, False, 0.5),\n",
    "    (\"Trigram (greedy)\",     trigram_model, True,  0.5),\n",
    "    (\"Trigram (temperature=0.5)\",trigram_model, False, 0.5),\n",
    "]\n",
    "\n",
    "for desc, model, greedy, temp in tests:\n",
    "    print(f\"--- {desc} ---\")\n",
    "    for seed in seeds:\n",
    "        generated = m_step_sentence_generation(\n",
    "            model=model,\n",
    "            prefix=prefix.copy(),\n",
    "            m=m,\n",
    "            greedy=greedy,\n",
    "            temperature=temp,\n",
    "            seed=seed\n",
    "        )\n",
    "        full_sentence = prefix + generated\n",
    "        print(f\"Seed {seed}: {' '.join(full_sentence)}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391d75ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
