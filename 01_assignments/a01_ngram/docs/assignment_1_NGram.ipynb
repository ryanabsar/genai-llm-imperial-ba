{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd0a34f",
   "metadata": {},
   "source": [
    "# Assignment 1: N-Gram Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda372bd",
   "metadata": {},
   "source": [
    "The **Berkeley Restaurant Project (BeRP)** corpus is a speech-dialogue dataset collected in the early 1990s to support a conversational system for answering restaurant‐related questions in Berkeley, California. The dataset contains nearly 9000 user utterances and more than 1700 vocabularies. In this assignment you will implement bigram and trigram language models on these queries, compute the probability of given sentences, and complete given prefixes with greedy decoding. Follow the hints in each code‐block title to fill in the missing parts of each function, then run each code-block to complete the assignment. Ensure your virtual environment is activated before launching the Jupyter Notebook. For detailed instructions, see `environment_setup.md`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f53725",
   "metadata": {},
   "source": [
    "## Step 1: Download the dataset\n",
    "Run the following code block. It automatically downloads the dataset (stored in `berp_dataset.txt`) and output some summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe83573-e5d8-4a63-9bd4-fb8b840e3eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import download_berp\n",
    "\n",
    "download_berp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7f65e2-6389-4985-8e68-aaba54054a75",
   "metadata": {},
   "source": [
    "## Step 2: Implement the N-gram Language Model Class\n",
    "\n",
    "Natural language data is sparse: many valid word sequences never appear in your training corpus, which would give them zero probability under a naïve count-based model. To address this, we apply smoothing when computing probability distributions:\n",
    "\n",
    "- **Laplace (add-one) smoothing**  \n",
    "  Add 1 to each N-gram count, and increase each prefix count by the vocabulary size V.  \n",
    "\n",
    "- **Additive smoothing**  \n",
    "  More flexible: add a small constant α (e.g. 0.1) to each count, and increase each prefix count by αV:  \n",
    "\n",
    "When generating text, you convert these probabilities into a choice. We implement two sampling methods:\n",
    "\n",
    "- **Temperature sampling** re-scales the distribution by raising each probability to the power of 1/T before renormalizing. T > 1 flattens the distribution (more exploration), while T < 1 sharpens it (more exploitation).\n",
    "\n",
    "- **Greedy sampling** always selects the highest-probability next word, equivalent to temperature sampling in the limit as T $\\to$ 0, which collapses the distribution to its single most likely outcome.\n",
    "\n",
    "\n",
    "\n",
    "Below is the skeleton of the `NGram_LM` class. **Do not** change any existing lines, only fill in the sections marked: \n",
    "```\n",
    "############## YOUR CODE HERE ##############\n",
    "#                                           \n",
    "#                                           \n",
    "############## YOUR CODE HERE ##############\n",
    "```\n",
    "Some hints are provided above the marked sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ecf855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import load_data\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Optional\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "corpus = load_data()\n",
    "\n",
    "\n",
    "class NGram_LM:\n",
    "    \"\"\"\n",
    "    An N-gram language model \n",
    "\n",
    "    Usage example:\n",
    "      corpus = load_data()\n",
    "      model = NGram_LM(corpus, N=3)\n",
    "      prob_of_to = model.next_word_prob(next_word = 'to', prefix = ['i', 'want'])\n",
    "      generated_next_word = model.next_word_generation(prefix = ['i', 'want'], greedy = True)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        corpus: List[List[str]],\n",
    "        N: int = 2,\n",
    "    ):\n",
    "        self.N = N\n",
    "        self.corpus = corpus\n",
    "        self.ngram_counts = defaultdict(int)\n",
    "        \n",
    "        # Count occurrences of each N-gram in the corpus\n",
    "        # After counting, self.ngram_counts should map each N-gram tuple to its frequency\n",
    "        # For example, when N=2:\n",
    "        #   {('see', 'i'): 3,\n",
    "        #    ('i', 'want'): 773,\n",
    "        #    ('want', 'to'): 556, ...}\n",
    "        ############## YOUR CODE HERE ##############\n",
    "        #\n",
    "        # self.ngram_counts = ?\n",
    "        #\n",
    "        ############## YOUR CODE HERE ##############\n",
    "\n",
    "\n",
    "        self.prefix_counts = defaultdict(int)\n",
    "        # Count occurrences of each (N-1)-gram prefix\n",
    "        # After counting, self.prefix_counts should map each prefix tuple to its frequency\n",
    "        # For example, when N=2:\n",
    "        #   {('okay',): 152,\n",
    "        #    (\"let's\",): 271,\n",
    "        #    ('see',): 84,\n",
    "        #    ('i',): 2844, ...}\n",
    "        ############## YOUR CODE HERE ##############\n",
    "        #\n",
    "        # self.prefix_counts = ?\n",
    "        #\n",
    "        ############## YOUR CODE HERE ##############\n",
    "\n",
    "\n",
    "\n",
    "        # Vocabulary size\n",
    "        self.vocab = list(set(word for sentence in self.corpus for word in sentence))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def next_word_prob(self, next_word: str, prefix: List[str], smoothing: str = 'laplace') -> float:\n",
    "        \"\"\"Compute next word probability given prefix.\"\"\"\n",
    "        assert len(prefix) == self.N - 1, \"Prefix must be of length N-1\"\n",
    "        prefix = tuple(prefix)\n",
    "        ngram = prefix + (next_word,)\n",
    "\n",
    "\n",
    "        # Compute smoothed probability P(next_word | prefix)\n",
    "        # Use self.ngram_counts, self.prefix_counts, and the selected smoothing method\n",
    "        # the smoothing methods are provided in _laplace_smoothing and _additive_smoothing\n",
    "        ############## YOUR CODE HERE ##############\n",
    "        #\n",
    "        # next_word_probability = ?\n",
    "        #\n",
    "        ############## YOUR CODE HERE ##############\n",
    "\n",
    "        return next_word_probbaility\n",
    "        \n",
    "\n",
    "    def next_word_generation(self, prefix: list, temperature: float = 1.0, greedy: bool = False, smoothing: str = 'laplace', seed: int = 1234):\n",
    "        \n",
    "        assert len(prefix) == self.N - 1, \"Prefix must be of length N-1\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        prefix = tuple(prefix)\n",
    "        \n",
    "        \n",
    "        # sample the next word conditional on the prefix\n",
    "        # you need to implement the temperature sampling and greedy sampling\n",
    "        # return the sampled next_word\n",
    "        ############## YOUR CODE HERE ##############\n",
    "        #\n",
    "        # next_word = ?\n",
    "        #\n",
    "        ############## YOUR CODE HERE ##############\n",
    "\n",
    "\n",
    "        return next_word\n",
    "\n",
    "\n",
    "\n",
    "    def _laplace_smoothing(self, ngram_count: int, prefix_count: int) -> float:\n",
    "        \"\"\"Laplace smoothing (add-one).\"\"\"\n",
    "        return (ngram_count + 1) / (prefix_count + self.vocab_size)\n",
    "\n",
    "    def _additive_smoothing(self, ngram_count: int, prefix_count: int, alpha: float = 0.1) -> float:\n",
    "        \"\"\"Additive smoothing with parameter alpha.\"\"\"\n",
    "        return (ngram_count + alpha) / (prefix_count + alpha * self.vocab_size)\n",
    "\n",
    "\n",
    "# check the result\n",
    "bigram_model = NGram_LM(corpus, N=2)\n",
    "trigram_model = NGram_LM(corpus, N=3)\n",
    "print(bigram_model.next_word_prob(next_word = 'to', prefix = ['i', 'want']))\n",
    "print(bigram_model.next_word_generation(prefix = ['i', 'want'], greedy = True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1d1e89",
   "metadata": {},
   "source": [
    "## Step 3: Sentence Probability Evaluation\n",
    "\n",
    "Implement a function that computes the log-probability of a full sentence under an N-gram model. Then, using both bigram (N=2) and trigram (N=3) versions of your `NGram_LM`, calculate the log-probability of each sentence given the prefix `['i', 'want']`:\n",
    "\n",
    "1. \"i want to eat on saturday night\"  \n",
    "2. \"i want to go to a restaurant\"  \n",
    "4. \"i want its sushis\"  \n",
    "5. \"i want to eat on monday berkeley\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f842b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_log_probability(\n",
    "    model: NGram_LM,\n",
    "    prefix: List[str],\n",
    "    sentence: List[str],\n",
    "    smoothing: str = 'laplace'\n",
    ") -> float:\n",
    "    \n",
    "    # Compute the total log-probability of `sentence` under `model`, starting from the given `prefix` and using the specified smoothing.\n",
    "    # Hint: accumulate log(next_word_prob) for each word in `sentence`\n",
    "    ############## YOUR CODE HERE ##############\n",
    "    #\n",
    "    # log_prob = ?\n",
    "    #\n",
    "    ############## YOUR CODE HERE ##############\n",
    "    return log_prob\n",
    "\n",
    "\n",
    "prefix = ['i', 'want']\n",
    "sentences = [\n",
    "    \"i want to eat on saturday night\",\n",
    "    \"i want to go to a restaurant\",\n",
    "    \"i want its sushis\",\n",
    "    \"i want to eat on monday berkeley\",\n",
    "]\n",
    "\n",
    "for model_name, model in [('Bigram', bigram_model), ('Trigram', trigram_model)]:\n",
    "    print(f\"--- {model_name} Model ---\")\n",
    "    for s in sentences:\n",
    "        words = s.split()[len(prefix):]\n",
    "        log_p = sentence_log_probability(model, prefix, words)\n",
    "        print(f\"Sentence: '{s}'\\nLog-Probability: {log_p:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd726d6",
   "metadata": {},
   "source": [
    "## Step 4: Next-Word Generation with Greedy and Temperature Sampling\n",
    "\n",
    "Create a function that, given a prefix, produces a sequence of next words using your `NGram_LM`. Then, for both the bigram (N=2) and trigram (N=3) models, generate five-word continuations of the prefix `['i', 'want']` using (a) greedy sampling and (b) temperature-based sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dea7605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def m_step_sentence_generation(model: NGram_LM, prefix: list, m: int, greedy: bool = False, temperature: float = 1.0, seed: int = 1234):\n",
    "    \n",
    "    \n",
    "    # Generate m next words from `model`, starting from `prefix` and using the specified sampling method\n",
    "    ############## YOUR CODE HERE ##############\n",
    "    #\n",
    "    # generated_sentence = ?\n",
    "    #\n",
    "    ############## YOUR CODE HERE ##############\n",
    "    return generated_sentence[len(list(prefix)):]\n",
    "\n",
    "\n",
    "prefix = ['i', 'want']\n",
    "m = 5\n",
    "seeds = [1, 2, 3]\n",
    "\n",
    "# Scenarios: (description, model, greedy flag, temperature)\n",
    "tests = [\n",
    "    (\"Bigram (greedy)\",     bigram_model, True,  0.5),\n",
    "    (\"Bigram (temperature=0.5)\", bigram_model, False, 0.5),\n",
    "    (\"Trigram (greedy)\",     trigram_model, True,  0.5),\n",
    "    (\"Trigram (temperature=0.5)\",trigram_model, False, 0.5),\n",
    "]\n",
    "\n",
    "for desc, model, greedy, temp in tests:\n",
    "    print(f\"--- {desc} ---\")\n",
    "    for seed in seeds:\n",
    "        generated = m_step_sentence_generation(\n",
    "            model=model,\n",
    "            prefix=prefix.copy(),\n",
    "            m=m,\n",
    "            greedy=greedy,\n",
    "            temperature=temp,\n",
    "            seed=seed\n",
    "        )\n",
    "        full_sentence = prefix + generated\n",
    "        print(f\"Seed {seed}: {' '.join(full_sentence)}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e30d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575956ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
